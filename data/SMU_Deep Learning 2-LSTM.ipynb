{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "### Introduction\n",
    "- RNNs are type of Deep Learning models with built-in feedback mechanism. \n",
    "- The output of a particular layer can be **re-fed** as the input in order to predict the output. \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/132912049-167cf37e-66a0-4b54-8024-183ab7785398.png)\n",
    "\n",
    "[source](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "- A look at detailed when we unroll the RNN loop:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/132911838-0ce7eb99-fd60-44c7-b554-d176fdb45f8b.png)\n",
    "\n",
    "\n",
    "### Types of RNN\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/132903689-398ef108-660d-47ba-ae46-b783f203e307.png)\n",
    "\n",
    "### Applications\n",
    "- It is specifically designed for Sequential problem **Weather forecast, Stock forecast, Image captioning, Natural Language Processing, Speech/Voice Recognition**\n",
    "\n",
    "### Some Disadvantages of RNN: \n",
    "- Computationally Expensive and large memory requested\n",
    "- RNN is sensitive to changes in parameters and having **Exploding Gradient** or **Vanishing Gradient**\n",
    "- In order to resolve the gradient problem of RNN, a method Long-Short Term Memory (LSTM) is proposed.\n",
    "\n",
    "In this limited workshop, we only cover LSTM for timeseries forecast problem.\n",
    "\n",
    "# Long-Short Term Memory model - LSTM\n",
    "### Introduction\n",
    "- LSTMs are a special kind of RNN â€” capable of learning long-term dependencies by remembering information for long periods is the default behavior.\n",
    "- They were introduced by Hochreiter & Schmidhuber (1997) and were refined and popularized by many people\n",
    "- LSTMs are explicitly designed to avoid the long-term dependency problem.\n",
    "\n",
    "### Comparison between traditional RNN and LSTM\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/132913273-1b7d4765-a8f2-4f2d-b3b9-6910d5d15807.png)\n",
    "\n",
    "### Step by step walkthrought LSTM:\n",
    "[Link](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "# Hands-on exercise on application of LSTM in temperature forecast\n",
    "Here, we will access Keras LSTM to forecast temperature at site name Jena (Germany), given information of temperature and other climate variables.\n",
    "The tutorial following the [keras website](https://keras.io/examples/timeseries/timeseries_weather_forecasting/), but rewritten in a simpler way for easy understanding.\n",
    "\n",
    "### Climate Data\n",
    "- Download data from [this link](https://drive.google.com/file/d/1vFs6uHrg24nmpOYuTKrtyW177Ik_gGUy/view?usp=sharing) and upload to your same folder where you have this notebook\n",
    "- Single station named Jena station in Germany.\n",
    "- Data consists of 14 climate variables in every 10 minutes\n",
    "- Temporal timeframe 8 year: 01/01/2009 - 12/31/2016\n",
    "- Data description:\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/132914704-b2c7ee79-0c99-482a-abfd-cc4575dcfe1b.png)\n",
    "\n",
    "- Input variables: all 14 climate variables including Temperature\n",
    "- Output or target variable: Temperature at later date\n",
    "\n",
    "### Objective\n",
    "- Using data from previous 5 days, forecast temperature in the next 12 hours\n",
    "\n",
    "#### Loading library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from numpy import array    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Jena climate station data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"jena_climate_2009_2016.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for any missing value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Time          0\n",
      "p (mbar)           0\n",
      "T (degC)           0\n",
      "Tpot (K)           0\n",
      "Tdew (degC)        0\n",
      "rh (%)             0\n",
      "VPmax (mbar)       0\n",
      "VPact (mbar)       0\n",
      "VPdef (mbar)       0\n",
      "sh (g/kg)          0\n",
      "H2OC (mmol/mol)    0\n",
      "rho (g/m**3)       0\n",
      "wv (m/s)           0\n",
      "max. wv (m/s)      0\n",
      "wd (deg)           0\n",
      "dtype: int64\n",
      "Date Time          0\n",
      "p (mbar)           0\n",
      "T (degC)           0\n",
      "Tpot (K)           0\n",
      "Tdew (degC)        0\n",
      "rh (%)             0\n",
      "VPmax (mbar)       0\n",
      "VPact (mbar)       0\n",
      "VPdef (mbar)       0\n",
      "sh (g/kg)          0\n",
      "H2OC (mmol/mol)    0\n",
      "rho (g/m**3)       0\n",
      "wv (m/s)           0\n",
      "max. wv (m/s)      0\n",
      "wd (deg)           0\n",
      "dtype: int64\n",
      "Date Time          01.01.2009 00:10:00\n",
      "p (mbar)                         913.6\n",
      "T (degC)                        -23.01\n",
      "Tpot (K)                         250.6\n",
      "Tdew (degC)                     -25.01\n",
      "rh (%)                           12.95\n",
      "VPmax (mbar)                      0.95\n",
      "VPact (mbar)                      0.79\n",
      "VPdef (mbar)                         0\n",
      "sh (g/kg)                          0.5\n",
      "H2OC (mmol/mol)                    0.8\n",
      "rho (g/m**3)                   1059.45\n",
      "wv (m/s)                         -9999\n",
      "max. wv (m/s)                    -9999\n",
      "wd (deg)                             0\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "print(df.isna().sum())\n",
    "print(df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values for wv and max. wv (denoted by -9999). Therefore we need to convert -9999 to nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only Temperature. atmospheric pressure (p) and relative humidity which has no missing values as input predictors\n",
    "selected_col = [0,1,2,5] \n",
    "df = df.iloc[:,selected_col].set_index([\"Date Time\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data partitioning\n",
    "\n",
    "- Data was collected at interval 10 minutes or 6 times an hour. Thus, resample the input data to hourly with the **sampling_rate** argument: **step=6**\n",
    "- Using historical data of 5 days in the past: 5 x 24 x 6 = **720 data points**\n",
    "- To forecast temperature in the next 12 hours: 12 x 6 = **72 data points**\n",
    "- Data partition to **70% training** and **30% testing** in order of time\n",
    "- For Neural Network, following parameters are pre-selected:\n",
    "   - **Learning rate** = 0.001\n",
    "   - **Batch size** = 256 (Batch size is the number of samples that usually pass through the neural network at one time)\n",
    "   - **Epoch** = 10 (Epoch is the number of times that the entire dataset pass through the neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.7\n",
    "train_split = int(split_fraction * int(df.shape[0]))\n",
    "\n",
    "step = 6 \n",
    "past = 720\n",
    "future = 72\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 256\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As input data has different range, so there would be the need for **standardization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:10:00</th>\n",
       "      <td>0.814939</td>\n",
       "      <td>0.248632</td>\n",
       "      <td>0.923033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:20:00</th>\n",
       "      <td>0.815430</td>\n",
       "      <td>0.242163</td>\n",
       "      <td>0.924182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:30:00</th>\n",
       "      <td>0.815037</td>\n",
       "      <td>0.240504</td>\n",
       "      <td>0.929925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:40:00</th>\n",
       "      <td>0.814840</td>\n",
       "      <td>0.243822</td>\n",
       "      <td>0.933372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:50:00</th>\n",
       "      <td>0.814840</td>\n",
       "      <td>0.244485</td>\n",
       "      <td>0.932223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     p (mbar)  T (degC)    rh (%)\n",
       "Date Time                                        \n",
       "01.01.2009 00:10:00  0.814939  0.248632  0.923033\n",
       "01.01.2009 00:20:00  0.815430  0.242163  0.924182\n",
       "01.01.2009 00:30:00  0.815037  0.240504  0.929925\n",
       "01.01.2009 00:40:00  0.814840  0.243822  0.933372\n",
       "01.01.2009 00:50:00  0.814840  0.244485  0.932223"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_features = pd.DataFrame(scale.fit_transform(df))\n",
    "scaled_features.columns = df.columns\n",
    "scaled_features.index = df.index\n",
    "\n",
    "# The split of training and testing data must follow the sequential\n",
    "train_data = scaled_features[0:train_split]\n",
    "test_data =  scaled_features[train_split:]\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:10:00</th>\n",
       "      <td>0.814939</td>\n",
       "      <td>0.248632</td>\n",
       "      <td>0.923033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:20:00</th>\n",
       "      <td>0.815430</td>\n",
       "      <td>0.242163</td>\n",
       "      <td>0.924182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:30:00</th>\n",
       "      <td>0.815037</td>\n",
       "      <td>0.240504</td>\n",
       "      <td>0.929925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:40:00</th>\n",
       "      <td>0.814840</td>\n",
       "      <td>0.243822</td>\n",
       "      <td>0.933372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01.01.2009 00:50:00</th>\n",
       "      <td>0.814840</td>\n",
       "      <td>0.244485</td>\n",
       "      <td>0.932223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     p (mbar)  T (degC)    rh (%)\n",
       "Date Time                                        \n",
       "01.01.2009 00:10:00  0.814939  0.248632  0.923033\n",
       "01.01.2009 00:20:00  0.815430  0.242163  0.924182\n",
       "01.01.2009 00:30:00  0.815037  0.240504  0.929925\n",
       "01.01.2009 00:40:00  0.814840  0.243822  0.933372\n",
       "01.01.2009 00:50:00  0.814840  0.244485  0.932223"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting input/output for training/testing dataset:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43855029/133829483-8190525b-d278-497c-af5c-73f5a372855d.png)\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph could be interpreted that:\n",
    "- The entire dataset is splitted to training and testing set based on the value of train_split\n",
    "- The X_train contains all input values [p (mbar), T (degC), rh (%)] and the y_train is [T (degC)]. Same for X_test, y_test.\n",
    "- past = 720 means 720 time steps of 10 minutes interval, which is equivalent to 5 days in the past. Here we use inputs from the past 5 days to predict output for the next 12 hours\n",
    "- future = 72 means the next 12 hours values are ignored. The forecast is for the next 12 hours value only.\n",
    "- 1 batch:  Input shape: (256, 120, 3); Target shape: (256,)\n",
    "- 1 batch contains 256 random samples (defined manually) of input X and Target y. Each random sample contain 120 values of X [p (mbar), T (degC), rh (%)] (120=720/6), converted from 10 minutes interval to 1 hour interval and 1 value of y [T (degC)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ytrain = past + future\n",
    "end_ytrain = train_split + start_ytrain\n",
    "\n",
    "x_train = train_data\n",
    "y_train = scaled_features[start_ytrain:end_ytrain][\"T (degC)\"]\n",
    "\n",
    "sequence_length = int(past/step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ytest = end_ytrain\n",
    "end_ytest = len(test_data) - past - future\n",
    "\n",
    "x_test = test_data.iloc[:end_ytest,:]\n",
    "y_test = scaled_features.iloc[start_ytest:][\"T (degC)\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125374,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training data set, the updated keras (with tensorflow version 2.3 and above) has built-in function to prepare for time series modeling using given batch size and the length for historical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Using Keras to split training/testing data to different batch:\n",
    "Here, we utilize the preprocessing time series feature of keras to split training/testing data into different batch:\n",
    "\n",
    "##### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate = step,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (256, 120, 3)\n",
      "Target shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "    print(\"Input shape:\", inputs.numpy().shape)\n",
    "    print(\"Target shape:\", targets.numpy().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (256, 120, 3)\n",
      "Target shape: (256,)\n",
      "Input shape: (256, 120, 3)\n",
      "Target shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset_test.take(2):\n",
    "    inputs_test, targets_test = batch\n",
    "    print(\"Input shape:\", inputs_test.numpy().shape)\n",
    "    print(\"Target shape:\", targets_test.numpy().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Deep learning model with LSTM framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "lstm_out = keras.layers.LSTM(32, activation=\"relu\")(inputs)\n",
    "outputs = keras.layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create callback Object for tensorboard visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tb_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=\"logs_LSTM/\", histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the LSTM model and vaidate with testing data set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_test,\n",
    "    callbacks=[tb_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Training & Testing loss with 10 different epoches?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Load the LSTM trained model\n",
    "\n",
    "Save LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_Jena.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('LSTM_Jena.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Prediction\n",
    "Modifying the given [code](https://keras.io/examples/timeseries/timeseries_weather_forecasting/) to make predictions for 5 sets of values from validation set:\n",
    "\n",
    "First, we need to create a rescale function back to original scale for T (degC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create transformation function to rescale back to original\n",
    "scaleT = MinMaxScaler(feature_range=(0,1))\n",
    "scaleT.fit_transform(pd.DataFrame(dfnew[:][\"T (degC)\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply plotting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.ylabel(\"T (degC)\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for x, y in dataset_test.take(5):\n",
    "    show_plot(\n",
    "        #[x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        [scaleT.inverse_transform(pd.DataFrame(x[0][:, 1])),\n",
    "         scaleT.inverse_transform(pd.DataFrame(pd.Series(y[0].numpy()))),\n",
    "         scaleT.inverse_transform(pd.DataFrame(model.predict(x)[0]))],         \n",
    "        12,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using all input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_test.take(10):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_SKLN",
   "language": "python",
   "name": "ml_skln"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
